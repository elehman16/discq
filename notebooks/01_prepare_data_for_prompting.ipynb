{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import src.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the df data is stored \n",
    "DF_DATA_LOCATION = \"../data/discq_questions_final.csv\"\n",
    "\n",
    "# Where to load the text files from\n",
    "RELATION_TXT_FOLDER_LOC = \"../data/relations_txt\"\n",
    "\n",
    "# Where to save the preprocessed data to\n",
    "SAVE_LOCATION = \"../data/clinical_qg_3_prior_2_future\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.t0_code.prepare_data_for_t0 import build_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DF_DATA_LOCATION)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = df.iloc[0].question\n",
    "trigger = df.iloc[0].reasoning\n",
    "doc_id = df.iloc[0].id\n",
    "\n",
    "shift = len(doc_id) - 1\n",
    "span = df.iloc[0].start_index - shift, df.iloc[0].end_index - shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/relations_txt/{doc_id}\", \"r\") as f:\n",
    "    doc_text = f.read()\n",
    "\n",
    "\n",
    "def extract_sentence_with_trigger(doc_text, trigger_span, expected_trigger_text=None, n_prior_sentences=0, n_future_sentences=0):\n",
    "    \"\"\"\n",
    "    Extracts the sentence with the trigger.\n",
    "    \n",
    "    If expected_trigger_text is not None,\n",
    "        checks if the trigger is the expected_trigger_text and tries to find it in the neighboring sentences (plus minus 20 sentences).\n",
    "        If the trigger is not found, raises RuntimeError.\n",
    "\n",
    "    Args:\n",
    "        n_future_sentences: Number of sentences to extract after the trigger.\n",
    "        n_prior_sentences: Number of sentences to extract before the trigger.\n",
    "    \"\"\"\n",
    "    doc_sentences = doc_text.splitlines(keepends=True)\n",
    "    # doc_sentences = [t for t in doc_text.split(\"\\n\") if len(t) > 0]\n",
    "    sentence_lengths = [len(s) for s in doc_sentences]\n",
    "\n",
    "    current_index = 0\n",
    "    sentence_index = None\n",
    "    for i, l in enumerate(sentence_lengths):\n",
    "        if trigger_span[0] <= current_index + l:\n",
    "            sentence_index = i\n",
    "            break\n",
    "        current_index += l\n",
    "\n",
    "    if sentence_index is None:\n",
    "        raise Exception(\"Could not find sentence with trigger by span\")\n",
    "\n",
    "    if expected_trigger_text is not None:\n",
    "        if expected_trigger_text not in doc_sentences[sentence_index]:\n",
    "            # Try to find the trigger in the neighboring sentences\n",
    "            for i in range(1, 20):\n",
    "                if expected_trigger_text in doc_sentences[sentence_index - i]:\n",
    "                    sentence_index = sentence_index - i\n",
    "                    break\n",
    "                if expected_trigger_text in doc_sentences[sentence_index + i]:\n",
    "                    sentence_index = sentence_index + i\n",
    "                    break\n",
    "            else:\n",
    "                raise RuntimeError(f\"Could not find expected trigger text {expected_trigger_text} in the document with name {doc_id}\")\n",
    "    \n",
    "    sentence_index_start = max(0, sentence_index - n_prior_sentences)\n",
    "    sentence_index_end = min(len(doc_sentences), sentence_index + n_future_sentences + 1)\n",
    "    sentence = \" \".join(doc_sentences[sentence_index_start:sentence_index_end])\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = extract_sentence_with_trigger(doc_text, span, n_prior_sentences=3, n_future_sentences=1)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''{text}\\nAfter reading the above EMR, what question do you have about \"{trigger}\"?\\nQuestion:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.format(text=sentence, trigger=trigger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building pre-processor based on the above logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    sql_file_path,\n",
    "    documents_folder,\n",
    "    n_prior_sentences=0,\n",
    "    n_future_sentences=0,\n",
    "    split_questions=False,\n",
    "    verbosity=1,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    seed=0,\n",
    "):\n",
    "\n",
    "    df = utils.load_df(sql_file_path)\n",
    "\n",
    "    tr_ids, val_ids, test_ids = utils.split_ids(list(set(df.id.values)), seed=0, val_split=0.15, test_split=0.15)\n",
    "\n",
    "    # Create each df\n",
    "    tr_df = df[df.id.isin(tr_ids)]\n",
    "    vl_df = df[df.id.isin(val_ids)]\n",
    "    te_df = df[df.id.isin(test_ids)]\n",
    "\n",
    "    dataset_dict = {}\n",
    "\n",
    "    n_errors = 0\n",
    "    for split_name, split_df in zip([\"train\", \"validation\", \"test\"], [tr_df, vl_df, te_df]):\n",
    "        dataset = []\n",
    "\n",
    "        for _, row in tqdm(split_df.iterrows(), total=len(df)):\n",
    "            with open(os.path.join(documents_folder, row.id), \"r\") as f:\n",
    "                doc_text = f.read()\n",
    "\n",
    "            shift = len(row.id) - 1\n",
    "            span = row.start_index - shift, row.end_index - shift\n",
    "\n",
    "            # span_text = doc_text[span[0]:span[1]]\n",
    "            # if row.reasoning != span_text:\n",
    "            #     span_text = doc_text[span[0] + 1:span[1] + 1]\n",
    "            #     if row.reasoning != span_text:\n",
    "            #         raise RuntimeError(f\"Trigger `{row.reasoning}` does not match corresponding span text `{span_text}`\")\n",
    "            #     else:\n",
    "            #         span = span[0], span[1] + 1\n",
    "\n",
    "            try:\n",
    "                sentence = extract_sentence_with_trigger(\n",
    "                    doc_text=doc_text,\n",
    "                    trigger_span=span,\n",
    "                    n_prior_sentences=n_prior_sentences,\n",
    "                    n_future_sentences=n_future_sentences,\n",
    "                    expected_trigger_text=row.reasoning,\n",
    "                )\n",
    "            except RuntimeError as e:\n",
    "                n_errors += 1\n",
    "                if n_errors < 3 or verbosity > 2:\n",
    "                    if verbosity: print(e)\n",
    "                elif n_errors == 3:\n",
    "                    if verbosity > 0: print(\"Too many errors, not printing any more\")\n",
    "                continue\n",
    "\n",
    "            if split_questions:\n",
    "                questions = [q.strip() + \"?\" for q in row.question.split(\"?\") if len(q) > 1]\n",
    "                if len(questions) > 1:\n",
    "                    if verbosity > 1: print(\"Multiple questions found:\", questions)\n",
    "                for question in questions:\n",
    "                    dataset.append({\"sentence\": sentence, \"trigger\": row.reasoning, \"question\": question})\n",
    "            else:\n",
    "                dataset.append({\"sentence\": sentence, \"trigger\": row.reasoning, \"question\": row.question})\n",
    "\n",
    "        if n_errors > 0:\n",
    "            if verbosity > 0: print(f\"Found {n_errors} errors. These examples were not added to the dataset\")\n",
    "\n",
    "        dataset = pd.DataFrame(dataset)\n",
    "        dataset = datasets.Dataset.from_pandas(dataset)\n",
    "\n",
    "        dataset_dict[split_name] = dataset\n",
    "\n",
    "\n",
    "    return datasets.DatasetDict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(DF_DATA_LOCATION, RELATION_TXT_FOLDER_LOC, n_prior_sentences=3, n_future_sentences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8769b4c304ab808c269bb719a9c25ec935a2e5c6460d13032706e0f9d23a474d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
